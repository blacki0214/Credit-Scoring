{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fce080bd",
   "metadata": {},
   "source": [
    "# Advanced Feature Engineering for Credit Scoring - FIXED VERSION\n",
    "\n",
    "This notebook creates powerful domain-specific features to improve model performance.\n",
    "\n",
    "**Goal**: Improve ROC-AUC from 0.64 to 0.75+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48d8b0c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import lightgbm as lgb\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"âœ… Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2879a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "LOADING RAW DATA\n",
      "================================================================================\n",
      "\n",
      "Raw data shape: (307511, 66)\n",
      "Columns: 66\n",
      "Default rate: 8.07%\n",
      "\n",
      "Class distribution:\n",
      "TARGET\n",
      "False    282686\n",
      "True      24825\n",
      "Name: count, dtype: int64\n",
      "TARGET\n",
      "False    0.919271\n",
      "True     0.080729\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Raw data shape: (307511, 66)\n",
      "Columns: 66\n",
      "Default rate: 8.07%\n",
      "\n",
      "Class distribution:\n",
      "TARGET\n",
      "False    282686\n",
      "True      24825\n",
      "Name: count, dtype: int64\n",
      "TARGET\n",
      "False    0.919271\n",
      "True     0.080729\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Load Raw Data\n",
    "print(\"=\"*80)\n",
    "print(\"LOADING RAW DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "df = pd.read_parquet(\n",
    "    r\"C:\\Users\\Asus\\Documents\\GitHub\\Credit-Scoring\\data\\data-processing\\flat_table\\flat_credit_model_20251027_143321.parquet\"\n",
    ")\n",
    "\n",
    "print(f\"\\nRaw data shape: {df.shape}\")\n",
    "print(f\"Columns: {len(df.columns)}\")\n",
    "print(f\"Default rate: {df['TARGET'].mean()*100:.2f}%\")\n",
    "\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(df['TARGET'].value_counts())\n",
    "print(df['TARGET'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aee4ba54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "AVAILABLE COLUMNS IN DATASET\n",
      "================================================================================\n",
      "\n",
      "Total columns: 66\n",
      "\n",
      "All columns:\n",
      "  1. SK_ID_CURR\n",
      "  2. TARGET\n",
      "  3. age_years\n",
      "  4. employment_years\n",
      "  5. annuity_income_ratio\n",
      "  6. credit_income_ratio\n",
      "  7. goods_income_ratio\n",
      "  8. income_per_person\n",
      "  9. has_job_flag\n",
      "  10. raw_income_total\n",
      "  11. raw_credit_amt\n",
      "  12. raw_annuity_amt\n",
      "  13. raw_goods_price\n",
      "  14. raw_cnt_fam_members\n",
      "  15. raw_days_employed\n",
      "  16. app_missing_income_flag\n",
      "  17. app_missing_credit_flag\n",
      "  18. app_missing_annuity_flag\n",
      "  19. app_missing_goods_flag\n",
      "  20. total_credit_sum\n",
      "  21. total_credit_debt\n",
      "  22. total_utilization\n",
      "  23. active_loans_count\n",
      "  24. closed_loans_count\n",
      "  25. max_overdue_ratio\n",
      "  26. raw_bureau_records\n",
      "  27. bur_raw_total_credit_sum\n",
      "  28. bur_raw_total_credit_debt\n",
      "  29. raw_total_overdue_amount\n",
      "  30. raw_overdue_loans_count\n",
      "  31. raw_has_overdue_flag\n",
      "  32. cc_avg_utilization\n",
      "  33. cc_max_utilization\n",
      "  34. cc_payment_ratio\n",
      "  35. cc_total_months\n",
      "  36. cc_active_month_ratio\n",
      "  37. cc_has_overdue_flag\n",
      "  38. raw_cc_records\n",
      "  39. cc_raw_limit_avg\n",
      "  40. cc_raw_balance_avg\n",
      "  41. cc_raw_total_payment\n",
      "  42. cc_raw_total_drawings\n",
      "  43. cc_raw_overdue_months\n",
      "  44. cc_raw_max_dpd\n",
      "  45. cc_raw_invalid_limit_flag\n",
      "  46. dpd_mean\n",
      "  47. dpd_max\n",
      "  48. on_time_ratio\n",
      "  49. num_payments\n",
      "  50. dpd_gt30_flag\n",
      "  51. ins_payment_ratio\n",
      "  52. ins_payment_variance\n",
      "  53. ins_early_ratio\n",
      "  54. raw_instalments_count\n",
      "  55. raw_payments_count\n",
      "  56. ins_raw_total_instalment\n",
      "  57. ins_raw_total_payment\n",
      "  58. ins_raw_on_time_count\n",
      "  59. ins_raw_late_count\n",
      "  60. ins_raw_max_dpd\n",
      "  61. ins_raw_missing_amount_flag\n",
      "  62. ins_raw_missing_days_flag\n",
      "  63. missing_income_flag\n",
      "  64. missing_bureau_flag\n",
      "  65. missing_cc_flag\n",
      "  66. missing_installment_flag\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Check Available Columns\n",
    "print(\"=\"*80)\n",
    "print(\"AVAILABLE COLUMNS IN DATASET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nTotal columns: {len(df.columns)}\")\n",
    "print(f\"\\nAll columns:\")\n",
    "for i, col in enumerate(df.columns, 1):\n",
    "    print(f\"  {i}. {col}\")\n",
    "\n",
    "# Store column names for mapping\n",
    "all_columns = df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48baa1aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "INTELLIGENT COLUMN MAPPING\n",
      "================================================================================\n",
      "\n",
      "ğŸ“‹ Column Mapping:\n",
      "  âœ… income          â†’ raw_income_total\n",
      "  âœ… credit          â†’ raw_credit_amt\n",
      "  âœ… annuity         â†’ raw_annuity_amt\n",
      "  âœ… total_debt      â†’ total_credit_debt\n",
      "  âœ… age_years       â†’ age_years\n",
      "  âœ… employment      â†’ employment_years\n",
      "  âœ… dpd_max         â†’ dpd_max\n",
      "  âœ… dpd_mean        â†’ dpd_mean\n",
      "  âŒ dpd_std         â†’ NOT FOUND\n",
      "\n",
      "âœ… Can create debt burden features: True\n",
      "âœ… Can create payment behavior features: False\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Smart Column Mapper\n",
    "print(\"=\"*80)\n",
    "print(\"INTELLIGENT COLUMN MAPPING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def find_column(candidates, df_columns):\n",
    "    \"\"\"Find first matching column from candidate list\"\"\"\n",
    "    for candidate in candidates:\n",
    "        if candidate in df_columns:\n",
    "            return candidate\n",
    "    return None\n",
    "\n",
    "# Map required columns with multiple alternatives\n",
    "column_map = {\n",
    "    'income': find_column(['income_total', 'raw_income_total', 'AMT_INCOME_TOTAL'], df.columns),\n",
    "    'credit': find_column(['credit_amount', 'raw_credit_amt', 'AMT_CREDIT'], df.columns),\n",
    "    'annuity': find_column(['annuity_amount', 'raw_annuity_amt', 'AMT_ANNUITY'], df.columns),\n",
    "    'total_debt': find_column(['total_credit_debt', 'AMT_CREDIT_SUM_DEBT'], df.columns),\n",
    "    'age_years': find_column(['age_years', 'DAYS_BIRTH'], df.columns),\n",
    "    'employment': find_column(['employment_years', 'DAYS_EMPLOYED'], df.columns),\n",
    "    'dpd_max': find_column(['dpd_max', 'pos_sk_dpd_max'], df.columns),\n",
    "    'dpd_mean': find_column(['dpd_mean', 'pos_sk_dpd_mean'], df.columns),\n",
    "    'dpd_std': find_column(['dpd_std', 'pos_sk_dpd_std'], df.columns),\n",
    "}\n",
    "\n",
    "print(\"\\nğŸ“‹ Column Mapping:\")\n",
    "for key, value in column_map.items():\n",
    "    if value:\n",
    "        print(f\"  âœ… {key:15s} â†’ {value}\")\n",
    "    else:\n",
    "        print(f\"  âŒ {key:15s} â†’ NOT FOUND\")\n",
    "\n",
    "# Check which features we can create\n",
    "can_create_debt = all([column_map['income'], column_map['credit'], column_map['total_debt']])\n",
    "can_create_payment = all([column_map['dpd_max'], column_map['dpd_mean'], column_map['dpd_std']])\n",
    "\n",
    "print(f\"\\nâœ… Can create debt burden features: {can_create_debt}\")\n",
    "print(f\"âœ… Can create payment behavior features: {can_create_payment}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "92645591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FEATURE ENGINEERING: DEBT BURDEN\n",
      "================================================================================\n",
      "\n",
      "âœ… Created 5 debt burden features\n",
      "   Features: total_debt_burden, new_loan_burden, combined_debt_burden, income_to_loan_ratio, monthly_payment_burden\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Create Debt Burden Features\n",
    "if can_create_debt:\n",
    "    print(\"=\"*80)\n",
    "    print(\"FEATURE ENGINEERING: DEBT BURDEN\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    income = df[column_map['income']].replace(0, np.nan)\n",
    "    credit = df[column_map['credit']]\n",
    "    total_debt = df[column_map['total_debt']]\n",
    "    \n",
    "    # 1. Total debt to income ratio\n",
    "    df['total_debt_burden'] = total_debt / income\n",
    "    \n",
    "    # 2. New loan to income ratio\n",
    "    df['new_loan_burden'] = credit / income\n",
    "    \n",
    "    # 3. Combined debt burden\n",
    "    df['combined_debt_burden'] = (total_debt + credit) / income\n",
    "    \n",
    "    # 4. Income to loan ratio (inverse - higher is better)\n",
    "    df['income_to_loan_ratio'] = income / credit.replace(0, np.nan)\n",
    "    \n",
    "    if column_map['annuity']:\n",
    "        annuity = df[column_map['annuity']]\n",
    "        # 5. Monthly payment burden\n",
    "        df['monthly_payment_burden'] = ((annuity + total_debt / 12) / (income / 12))\n",
    "    \n",
    "    # Clip extreme values\n",
    "    ratio_cols = ['total_debt_burden', 'new_loan_burden', 'combined_debt_burden', \n",
    "                  'income_to_loan_ratio']\n",
    "    if column_map['annuity']:\n",
    "        ratio_cols.append('monthly_payment_burden')\n",
    "    \n",
    "    df[ratio_cols] = df[ratio_cols].clip(lower=-10, upper=10)\n",
    "    \n",
    "    print(f\"\\nâœ… Created {len(ratio_cols)} debt burden features\")\n",
    "    print(f\"   Features: {', '.join(ratio_cols)}\")\n",
    "else:\n",
    "    print(\"âš ï¸ Skipping debt burden features - required columns not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f4b7337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ Skipping payment behavior features - required columns not found\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Create Payment Behavior Features\n",
    "if can_create_payment:\n",
    "    print(\"=\"*80)\n",
    "    print(\"FEATURE ENGINEERING: PAYMENT BEHAVIOR\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    dpd_max = df[column_map['dpd_max']]\n",
    "    dpd_mean = df[column_map['dpd_mean']]\n",
    "    dpd_std = df[column_map['dpd_std']]\n",
    "    \n",
    "    # 1. Payment deterioration\n",
    "    df['payment_deterioration'] = dpd_max - dpd_mean\n",
    "    \n",
    "    # 2. Payment consistency\n",
    "    df['payment_consistency'] = 1 / (dpd_std + 1)\n",
    "    \n",
    "    # 3. Serious delinquency flag\n",
    "    df['serious_delinquency'] = (dpd_max > 60).astype(int)\n",
    "    \n",
    "    # 4. Any delinquency flag\n",
    "    df['any_delinquency'] = (dpd_max > 0).astype(int)\n",
    "    \n",
    "    # 5. DPD trend (max vs mean)\n",
    "    df['dpd_trend'] = dpd_max / (dpd_mean + 1)\n",
    "    \n",
    "    payment_features = ['payment_deterioration', 'payment_consistency', \n",
    "                       'serious_delinquency', 'any_delinquency', 'dpd_trend']\n",
    "    \n",
    "    print(f\"\\nâœ… Created {len(payment_features)} payment behavior features\")\n",
    "    print(f\"   Features: {', '.join(payment_features)}\")\n",
    "else:\n",
    "    print(\"âš ï¸ Skipping payment behavior features - required columns not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "59b74866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FEATURE ENGINEERING: DEMOGRAPHICS\n",
      "================================================================================\n",
      "\n",
      "âœ… Created 4 demographic features\n",
      "   Features: age_group, prime_age, employment_stable, age_adjusted_income\n",
      "\n",
      "âœ… Created 4 demographic features\n",
      "   Features: age_group, prime_age, employment_stable, age_adjusted_income\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Create Age & Employment Features\n",
    "print(\"=\"*80)\n",
    "print(\"FEATURE ENGINEERING: DEMOGRAPHICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "created_demo_features = []\n",
    "\n",
    "if column_map['age_years']:\n",
    "    age = df[column_map['age_years']]\n",
    "    \n",
    "    # 1. Age groups\n",
    "    df['age_group'] = pd.cut(age, bins=[0, 25, 35, 45, 55, 100], \n",
    "                             labels=['<25', '25-35', '35-45', '45-55', '55+'])\n",
    "    df['age_group'] = df['age_group'].cat.codes\n",
    "    \n",
    "    # 2. Prime age flag (30-50 years)\n",
    "    df['prime_age'] = ((age >= 30) & (age <= 50)).astype(int)\n",
    "    \n",
    "    created_demo_features.extend(['age_group', 'prime_age'])\n",
    "\n",
    "if column_map['employment']:\n",
    "    employment = df[column_map['employment']]\n",
    "    \n",
    "    # 3. Employment stability\n",
    "    df['employment_stable'] = (employment > 2).astype(int)  # >2 years\n",
    "    \n",
    "    created_demo_features.append('employment_stable')\n",
    "\n",
    "if column_map['age_years'] and column_map['income']:\n",
    "    # 4. Age-adjusted income\n",
    "    df['age_adjusted_income'] = df[column_map['income']] / (age + 1)\n",
    "    created_demo_features.append('age_adjusted_income')\n",
    "\n",
    "if created_demo_features:\n",
    "    print(f\"\\nâœ… Created {len(created_demo_features)} demographic features\")\n",
    "    print(f\"   Features: {', '.join(created_demo_features)}\")\n",
    "else:\n",
    "    print(\"âš ï¸ No demographic features created - columns not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b34cc2ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FEATURE ENGINEERING: INTERACTIONS\n",
      "================================================================================\n",
      "\n",
      "âœ… Created 2 interaction features\n",
      "   Features: age_income_interaction, credit_employment_risk\n",
      "\n",
      "FEATURE ENGINEERING: INTERACTIONS\n",
      "================================================================================\n",
      "\n",
      "âœ… Created 2 interaction features\n",
      "   Features: age_income_interaction, credit_employment_risk\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Create Interaction Features\n",
    "print(\"=\"*80)\n",
    "print(\"FEATURE ENGINEERING: INTERACTIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "interaction_features = []\n",
    "\n",
    "# Income x Age interaction\n",
    "if column_map['income'] and column_map['age_years']:\n",
    "    df['age_income_interaction'] = df[column_map['age_years']] * np.log1p(df[column_map['income']])\n",
    "    interaction_features.append('age_income_interaction')\n",
    "\n",
    "# DPD x Debt interaction\n",
    "if can_create_payment and 'total_debt_burden' in df.columns:\n",
    "    df['dpd_debt_risk'] = df[column_map['dpd_max']] * df['total_debt_burden']\n",
    "    interaction_features.append('dpd_debt_risk')\n",
    "\n",
    "# Credit amount x Employment\n",
    "if column_map['credit'] and column_map['employment']:\n",
    "    df['credit_employment_risk'] = df[column_map['credit']] / (df[column_map['employment']] + 1)\n",
    "    interaction_features.append('credit_employment_risk')\n",
    "\n",
    "if interaction_features:\n",
    "    print(f\"\\nâœ… Created {len(interaction_features)} interaction features\")\n",
    "    print(f\"   Features: {', '.join(interaction_features)}\")\n",
    "else:\n",
    "    print(\"âš ï¸ No interaction features created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a43d60b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FEATURE ENGINEERING: MISSING VALUE FLAGS\n",
      "================================================================================\n",
      "\n",
      "âœ… Created 3 missing value indicators\n",
      "   Features: employment_years_missing, raw_annuity_amt_missing, total_credit_debt_missing\n",
      "\n",
      "âœ… Created 3 missing value indicators\n",
      "   Features: employment_years_missing, raw_annuity_amt_missing, total_credit_debt_missing\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Create Missing Value Indicators\n",
    "print(\"=\"*80)\n",
    "print(\"FEATURE ENGINEERING: MISSING VALUE FLAGS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Key columns to track missing values\n",
    "missing_track_cols = [col for col in [column_map['employment'], column_map['annuity'], \n",
    "                                       column_map['total_debt']] if col is not None]\n",
    "\n",
    "missing_features = []\n",
    "for col in missing_track_cols:\n",
    "    if df[col].isnull().sum() > 0:\n",
    "        feature_name = f'{col}_missing'\n",
    "        df[feature_name] = df[col].isnull().astype(int)\n",
    "        missing_features.append(feature_name)\n",
    "\n",
    "if missing_features:\n",
    "    print(f\"\\nâœ… Created {len(missing_features)} missing value indicators\")\n",
    "    print(f\"   Features: {', '.join(missing_features)}\")\n",
    "else:\n",
    "    print(\"âœ… No significant missing values to track\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "905ce24b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FEATURE ENGINEERING: COMPOSITE RISK SCORE\n",
      "================================================================================\n",
      "\n",
      "âœ… Created composite_risk_score\n",
      "   Based on 2 components\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Create Risk Score\n",
    "print(\"=\"*80)\n",
    "print(\"FEATURE ENGINEERING: COMPOSITE RISK SCORE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Calculate composite risk score based on available features\n",
    "risk_components = []\n",
    "\n",
    "if 'total_debt_burden' in df.columns:\n",
    "    risk_components.append(df['total_debt_burden'].fillna(0))\n",
    "\n",
    "if 'any_delinquency' in df.columns:\n",
    "    risk_components.append(df['any_delinquency'] * 2)  # Weight delinquency higher\n",
    "\n",
    "if 'employment_stable' in df.columns:\n",
    "    risk_components.append((1 - df['employment_stable']))  # Unstable = higher risk\n",
    "\n",
    "if risk_components:\n",
    "    df['composite_risk_score'] = sum(risk_components) / len(risk_components)\n",
    "    print(\"\\nâœ… Created composite_risk_score\")\n",
    "    print(f\"   Based on {len(risk_components)} components\")\n",
    "else:\n",
    "    print(\"âš ï¸ Insufficient features for risk score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "12a609cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FEATURE ENGINEERING SUMMARY\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š Feature Statistics:\n",
      "   Original features: 62\n",
      "   New features created: 15\n",
      "   Total features will be: 77\n",
      "\n",
      "âœ¨ New Features Created:\n",
      "    1. total_debt_burden\n",
      "    2. new_loan_burden\n",
      "    3. combined_debt_burden\n",
      "    4. income_to_loan_ratio\n",
      "    5. monthly_payment_burden\n",
      "    6. age_group\n",
      "    7. prime_age\n",
      "    8. employment_stable\n",
      "    9. age_adjusted_income\n",
      "   10. age_income_interaction\n",
      "   11. credit_employment_risk\n",
      "   12. employment_years_missing\n",
      "   13. raw_annuity_amt_missing\n",
      "   14. total_credit_debt_missing\n",
      "   15. composite_risk_score\n",
      "\n",
      "ğŸ“ˆ Data Quality Check:\n",
      "   Total rows: 307,511\n",
      "   Missing values in new features:\n",
      "      total_debt_burden: 14.3%\n",
      "      combined_debt_burden: 14.3%\n",
      "      monthly_payment_burden: 14.3%\n",
      "      credit_employment_risk: 18.0%\n",
      "\n",
      "ğŸ“Š Feature Statistics:\n",
      "   Original features: 62\n",
      "   New features created: 15\n",
      "   Total features will be: 77\n",
      "\n",
      "âœ¨ New Features Created:\n",
      "    1. total_debt_burden\n",
      "    2. new_loan_burden\n",
      "    3. combined_debt_burden\n",
      "    4. income_to_loan_ratio\n",
      "    5. monthly_payment_burden\n",
      "    6. age_group\n",
      "    7. prime_age\n",
      "    8. employment_stable\n",
      "    9. age_adjusted_income\n",
      "   10. age_income_interaction\n",
      "   11. credit_employment_risk\n",
      "   12. employment_years_missing\n",
      "   13. raw_annuity_amt_missing\n",
      "   14. total_credit_debt_missing\n",
      "   15. composite_risk_score\n",
      "\n",
      "ğŸ“ˆ Data Quality Check:\n",
      "   Total rows: 307,511\n",
      "   Missing values in new features:\n",
      "      total_debt_burden: 14.3%\n",
      "      combined_debt_burden: 14.3%\n",
      "      monthly_payment_burden: 14.3%\n",
      "      credit_employment_risk: 18.0%\n"
     ]
    }
   ],
   "source": [
    "# Cell 11: Summary of New Features\n",
    "print(\"=\"*80)\n",
    "print(\"FEATURE ENGINEERING SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Get original column count from v2 data\n",
    "original_data = joblib.load(r\"C:\\Users\\Asus\\Documents\\GitHub\\Credit-Scoring\\output\\models\\processed_data_lgbm_v2.pkl\")\n",
    "original_features = original_data[0].shape[1]\n",
    "\n",
    "# Get list of all new features\n",
    "all_new_features = [col for col in df.columns if col not in all_columns and col != 'TARGET']\n",
    "\n",
    "print(f\"\\nğŸ“Š Feature Statistics:\")\n",
    "print(f\"   Original features: {original_features}\")\n",
    "print(f\"   New features created: {len(all_new_features)}\")\n",
    "print(f\"   Total features will be: {original_features + len(all_new_features)}\")\n",
    "\n",
    "print(f\"\\nâœ¨ New Features Created:\")\n",
    "for i, feat in enumerate(all_new_features, 1):\n",
    "    print(f\"   {i:2d}. {feat}\")\n",
    "\n",
    "print(f\"\\nğŸ“ˆ Data Quality Check:\")\n",
    "print(f\"   Total rows: {len(df):,}\")\n",
    "print(f\"   Missing values in new features:\")\n",
    "for feat in all_new_features:\n",
    "    missing_pct = df[feat].isnull().mean() * 100\n",
    "    if missing_pct > 0:\n",
    "        print(f\"      {feat}: {missing_pct:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "33a657ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PREPARING ENHANCED DATASET\n",
      "================================================================================\n",
      "\n",
      "Handling missing values...\n",
      "\n",
      "âœ… Data preparation complete\n",
      "   Feature matrix shape: (307511, 79)\n",
      "   Target shape: (307511,)\n",
      "   Class distribution: Counter({False: 282686, True: 24825})\n",
      "\n",
      "âœ… Data preparation complete\n",
      "   Feature matrix shape: (307511, 79)\n",
      "   Target shape: (307511,)\n",
      "   Class distribution: Counter({False: 282686, True: 24825})\n"
     ]
    }
   ],
   "source": [
    "# Cell 12: Prepare Data for Training\n",
    "print(\"=\"*80)\n",
    "print(\"PREPARING ENHANCED DATASET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop(columns=['TARGET', 'SK_ID_CURR'] if 'SK_ID_CURR' in df.columns else ['TARGET'])\n",
    "y = df['TARGET']\n",
    "\n",
    "# Handle any remaining missing values\n",
    "print(f\"\\nHandling missing values...\")\n",
    "X = X.fillna(X.median())\n",
    "\n",
    "# Handle any infinite values\n",
    "X = X.replace([np.inf, -np.inf], np.nan)\n",
    "X = X.fillna(0)\n",
    "\n",
    "print(f\"\\nâœ… Data preparation complete\")\n",
    "print(f\"   Feature matrix shape: {X.shape}\")\n",
    "print(f\"   Target shape: {y.shape}\")\n",
    "print(f\"   Class distribution: {Counter(y)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "27efe961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CREATING TRAIN-TEST SPLIT\n",
      "================================================================================\n",
      "\n",
      "âœ… Data split complete:\n",
      "   Train: (246008, 79)\n",
      "   Test:  (61503, 79)\n",
      "\n",
      "   Train class distribution:\n",
      "      Class 0: 226,148 (91.9%)\n",
      "      Class 1: 19,860 (8.1%)\n",
      "\n",
      "âœ… Data split complete:\n",
      "   Train: (246008, 79)\n",
      "   Test:  (61503, 79)\n",
      "\n",
      "   Train class distribution:\n",
      "      Class 0: 226,148 (91.9%)\n",
      "      Class 1: 19,860 (8.1%)\n"
     ]
    }
   ],
   "source": [
    "# Cell 13: Train-Test Split\n",
    "print(\"=\"*80)\n",
    "print(\"CREATING TRAIN-TEST SPLIT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… Data split complete:\")\n",
    "print(f\"   Train: {X_train.shape}\")\n",
    "print(f\"   Test:  {X_test.shape}\")\n",
    "print(f\"\\n   Train class distribution:\")\n",
    "print(f\"      Class 0: {Counter(y_train)[0]:,} ({Counter(y_train)[0]/len(y_train)*100:.1f}%)\")\n",
    "print(f\"      Class 1: {Counter(y_train)[1]:,} ({Counter(y_train)[1]/len(y_train)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3ce23104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "QUICK BASELINE TEST WITH NEW FEATURES\n",
      "================================================================================\n",
      "\n",
      "ğŸ”„ Training quick baseline...\n",
      "[LightGBM] [Info] Number of positive: 19860, number of negative: 226148\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.088072 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12551\n",
      "[LightGBM] [Info] Number of data points in the train set: 246008, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080729 -> initscore=-2.432482\n",
      "[LightGBM] [Info] Start training from score -2.432482\n",
      "[LightGBM] [Info] Number of positive: 19860, number of negative: 226148\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.088072 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12551\n",
      "[LightGBM] [Info] Number of data points in the train set: 246008, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080729 -> initscore=-2.432482\n",
      "[LightGBM] [Info] Start training from score -2.432482\n",
      "\n",
      "ğŸ“Š Quick Test Results:\n",
      "   Old model AUC: 0.6465\n",
      "   New features AUC: 0.7078\n",
      "   Improvement: +9.47%\n",
      "\n",
      "âœ… SUCCESS! New features improved performance!\n",
      "\n",
      "ğŸ“Š Quick Test Results:\n",
      "   Old model AUC: 0.6465\n",
      "   New features AUC: 0.7078\n",
      "   Improvement: +9.47%\n",
      "\n",
      "âœ… SUCCESS! New features improved performance!\n"
     ]
    }
   ],
   "source": [
    "# Cell 14: Quick Baseline Test\n",
    "print(\"=\"*80)\n",
    "print(\"QUICK BASELINE TEST WITH NEW FEATURES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Train quick model to verify features work\n",
    "lgb_test = lgb.LGBMClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    num_leaves=31,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"\\nğŸ”„ Training quick baseline...\")\n",
    "lgb_test.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_prob_test = lgb_test.predict_proba(X_test)[:, 1]\n",
    "baseline_auc = roc_auc_score(y_test, y_prob_test)\n",
    "\n",
    "# Load old AUC for comparison\n",
    "old_metadata = joblib.load(r\"C:\\Users\\Asus\\Documents\\GitHub\\Credit-Scoring\\output\\models\\improved_model_metadata.pkl\")\n",
    "old_auc = old_metadata['roc_auc']\n",
    "\n",
    "improvement = ((baseline_auc - old_auc) / old_auc) * 100\n",
    "\n",
    "print(f\"\\nğŸ“Š Quick Test Results:\")\n",
    "print(f\"   Old model AUC: {old_auc:.4f}\")\n",
    "print(f\"   New features AUC: {baseline_auc:.4f}\")\n",
    "print(f\"   Improvement: {improvement:+.2f}%\")\n",
    "\n",
    "if baseline_auc > old_auc:\n",
    "    print(f\"\\nâœ… SUCCESS! New features improved performance!\")\n",
    "elif baseline_auc > 0.70:\n",
    "    print(f\"\\nâœ… GOOD! AUC > 0.70 with minimal training\")\n",
    "else:\n",
    "    print(f\"\\nâš ï¸ WARNING: Performance not improved yet\")\n",
    "    print(f\"   This may improve with full hyperparameter tuning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6e7045ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FEATURE IMPORTANCE ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š Top 20 Most Important Features:\n",
      "================================================================================\n",
      "   active_loans_count                     143.0\n",
      "   raw_goods_price                        134.0\n",
      "   raw_annuity_amt                        133.0\n",
      "   total_utilization                      131.0\n",
      "   raw_credit_amt                         127.0\n",
      "   total_credit_sum                       117.0\n",
      "   annuity_income_ratio                    97.0\n",
      "ğŸ†• credit_employment_risk                  89.0\n",
      "   ins_raw_total_instalment                88.0\n",
      "   on_time_ratio                           85.0\n",
      "ğŸ†• age_adjusted_income                     83.0\n",
      "   ins_raw_on_time_count                   71.0\n",
      "ğŸ†• age_income_interaction                  69.0\n",
      "   cc_max_utilization                      66.0\n",
      "   ins_raw_total_payment                   65.0\n",
      "   age_years                               64.0\n",
      "   employment_years                        62.0\n",
      "   cc_payment_ratio                        62.0\n",
      "   ins_payment_variance                    58.0\n",
      "   ins_early_ratio                         56.0\n",
      "\n",
      "ğŸ“ˆ New Feature Impact:\n",
      "   New features in top 20: 3/15\n",
      "   New features in top 50: 8/15\n",
      "   Total new features: 15\n"
     ]
    }
   ],
   "source": [
    "# Cell 15: Feature Importance Analysis\n",
    "print(\"=\"*80)\n",
    "print(\"FEATURE IMPORTANCE ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Get feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': lgb_test.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Highlight new features\n",
    "feature_importance['is_new'] = feature_importance['feature'].isin(all_new_features)\n",
    "\n",
    "print(f\"\\nğŸ“Š Top 20 Most Important Features:\")\n",
    "print(\"=\"*80)\n",
    "for idx, row in feature_importance.head(20).iterrows():\n",
    "    marker = \"ğŸ†•\" if row['is_new'] else \"  \"\n",
    "    print(f\"{marker} {row['feature']:35s} {row['importance']:8.1f}\")\n",
    "\n",
    "# Count new features in top rankings\n",
    "new_in_top20 = feature_importance.head(20)['is_new'].sum()\n",
    "new_in_top50 = feature_importance.head(50)['is_new'].sum()\n",
    "\n",
    "print(f\"\\nğŸ“ˆ New Feature Impact:\")\n",
    "print(f\"   New features in top 20: {new_in_top20}/{len(all_new_features)}\")\n",
    "print(f\"   New features in top 50: {new_in_top50}/{len(all_new_features)}\")\n",
    "print(f\"   Total new features: {len(all_new_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4b870f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SAVING ENHANCED DATA\n",
      "================================================================================\n",
      "\n",
      "âœ… Enhanced data saved successfully!\n",
      "   Data: processed_data_lgbm_v3.pkl\n",
      "   Metadata: enhanced_data_metadata.pkl\n",
      "\n",
      "ğŸ“Š Summary:\n",
      "   Total features: 79\n",
      "   New features: 15\n",
      "   Baseline AUC: 0.7078\n",
      "   Improvement: +9.47%\n",
      "\n",
      "ğŸ¯ Next Step: Re-run 04b_baseline_model_LightGBM_improved.ipynb\n",
      "\n",
      "âœ… Enhanced data saved successfully!\n",
      "   Data: processed_data_lgbm_v3.pkl\n",
      "   Metadata: enhanced_data_metadata.pkl\n",
      "\n",
      "ğŸ“Š Summary:\n",
      "   Total features: 79\n",
      "   New features: 15\n",
      "   Baseline AUC: 0.7078\n",
      "   Improvement: +9.47%\n",
      "\n",
      "ğŸ¯ Next Step: Re-run 04b_baseline_model_LightGBM_improved.ipynb\n"
     ]
    }
   ],
   "source": [
    "# Cell 16: Save Enhanced Data\n",
    "print(\"=\"*80)\n",
    "print(\"SAVING ENHANCED DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Save processed data\n",
    "output_path = r\"C:\\Users\\Asus\\Documents\\GitHub\\Credit-Scoring\\output\\models\\processed_data_lgbm_v3.pkl\"\n",
    "joblib.dump((X_train, X_test, y_train, y_test), output_path)\n",
    "\n",
    "# Save metadata\n",
    "metadata = {\n",
    "    'creation_date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'original_features': original_features,\n",
    "    'new_features_created': len(all_new_features),\n",
    "    'total_features': X_train.shape[1],\n",
    "    'new_feature_list': all_new_features,\n",
    "    'baseline_auc': baseline_auc,\n",
    "    'old_auc': old_auc,\n",
    "    'improvement_vs_old': improvement,\n",
    "    'train_shape': X_train.shape,\n",
    "    'test_shape': X_test.shape,\n",
    "    'class_distribution': {\n",
    "        'train': Counter(y_train),\n",
    "        'test': Counter(y_test)\n",
    "    }\n",
    "}\n",
    "\n",
    "metadata_path = r\"C:\\Users\\Asus\\Documents\\GitHub\\Credit-Scoring\\output\\models\\enhanced_data_metadata.pkl\"\n",
    "joblib.dump(metadata, metadata_path)\n",
    "\n",
    "print(f\"\\nâœ… Enhanced data saved successfully!\")\n",
    "print(f\"   Data: processed_data_lgbm_v3.pkl\")\n",
    "print(f\"   Metadata: enhanced_data_metadata.pkl\")\n",
    "print(f\"\\nğŸ“Š Summary:\")\n",
    "print(f\"   Total features: {X_train.shape[1]}\")\n",
    "print(f\"   New features: {len(all_new_features)}\")\n",
    "print(f\"   Baseline AUC: {baseline_auc:.4f}\")\n",
    "print(f\"   Improvement: {improvement:+.2f}%\")\n",
    "print(f\"\\nğŸ¯ Next Step: Re-run 04b_baseline_model_LightGBM_improved.ipynb\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
