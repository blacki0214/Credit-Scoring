{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83797aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "import joblib\n",
    "from collections import Counter\n",
    "\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acccbad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DATA LOADING\n",
      "================================================================================\n",
      "Raw data shape: (307511, 66)\n",
      "Columns: 66\n",
      "Memory usage: 139.89 MB\n",
      "\n",
      "Target distribution:\n",
      "TARGET\n",
      "False    282686\n",
      "True      24825\n",
      "Name: count, dtype: int64\n",
      "TARGET\n",
      "False    0.919271\n",
      "True     0.080729\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Load raw data\n",
    "df = pd.read_parquet(\n",
    "    r\"C:\\Users\\Asus\\Documents\\GitHub\\Credit-Scoring\\data\\data-processing\\flat_table\\flat_credit_model_20251027_143321.parquet\",\n",
    "    engine=\"fastparquet\"\n",
    ")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"DATA LOADING\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Raw data shape: {df.shape}\")\n",
    "print(f\"Columns: {len(df.columns)}\")\n",
    "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "# Check target distribution\n",
    "print(f\"\\nTarget distribution:\")\n",
    "print(df['TARGET'].value_counts())\n",
    "print(df['TARGET'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eaa81028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DATA QUALITY ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Columns with >50% missing values:\n",
      "                   Column  Missing_Count  Missing_Percentage\n",
      "         cc_payment_ratio         250066               81.32\n",
      "     cc_raw_total_payment         246451               80.14\n",
      "    cc_active_month_ratio         246202               80.06\n",
      "       cc_avg_utilization         221475               72.02\n",
      "       cc_max_utilization         221475               72.02\n",
      "       cc_raw_balance_avg         220606               71.74\n",
      "    cc_raw_total_drawings         220606               71.74\n",
      "cc_raw_invalid_limit_flag         220606               71.74\n",
      "          cc_total_months         220606               71.74\n",
      "           raw_cc_records         220606               71.74\n",
      "           cc_raw_max_dpd         220606               71.74\n",
      "      cc_has_overdue_flag         220606               71.74\n",
      "         cc_raw_limit_avg         220606               71.74\n",
      "    cc_raw_overdue_months         220606               71.74\n",
      "\n",
      "Total columns with >50% missing: 14\n",
      "Total columns with >90% missing: 0\n",
      "\n",
      "Duplicate rows: 0\n",
      "Constant features: 6\n"
     ]
    }
   ],
   "source": [
    "# Data Quality Analysis\n",
    "print(\"=\"*80)\n",
    "print(\"DATA QUALITY ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Missing values analysis\n",
    "missing_df = pd.DataFrame({\n",
    "    'Column': df.columns,\n",
    "    'Missing_Count': df.isnull().sum(),\n",
    "    'Missing_Percentage': (df.isnull().sum() / len(df) * 100).round(2)\n",
    "}).sort_values('Missing_Percentage', ascending=False)\n",
    "\n",
    "print(f\"\\nColumns with >50% missing values:\")\n",
    "high_missing = missing_df[missing_df['Missing_Percentage'] > 50]\n",
    "print(high_missing.head(20).to_string(index=False))\n",
    "\n",
    "print(f\"\\nTotal columns with >50% missing: {len(high_missing)}\")\n",
    "print(f\"Total columns with >90% missing: {len(missing_df[missing_df['Missing_Percentage'] > 90])}\")\n",
    "\n",
    "# Check for duplicate rows\n",
    "print(f\"\\nDuplicate rows: {df.duplicated().sum()}\")\n",
    "\n",
    "# Check for constant features\n",
    "constant_features = [col for col in df.columns if df[col].nunique() <= 1]\n",
    "print(f\"Constant features: {len(constant_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53b479f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FEATURE CLEANING\n",
      "================================================================================\n",
      "Dropping 7 columns:\n",
      "  - 0 high missing\n",
      "  - 6 constant\n",
      "  - SK_ID_CURR (ID column)\n",
      "\n",
      "‚úÖ Shape after cleaning: (307511, 59)\n"
     ]
    }
   ],
   "source": [
    "# Remove Low-Quality Features\n",
    "print(\"=\"*80)\n",
    "print(\"FEATURE CLEANING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Remove columns with >90% missing values\n",
    "threshold = 90\n",
    "cols_to_drop = missing_df[missing_df['Missing_Percentage'] > threshold]['Column'].tolist()\n",
    "\n",
    "# Remove constant features\n",
    "cols_to_drop.extend(constant_features)\n",
    "\n",
    "# Remove SK_ID_CURR (identifier, not feature)\n",
    "if 'SK_ID_CURR' in df.columns:\n",
    "    cols_to_drop.append('SK_ID_CURR')\n",
    "\n",
    "# Remove duplicates from list\n",
    "cols_to_drop = list(set(cols_to_drop))\n",
    "\n",
    "# Ensure TARGET is not dropped\n",
    "if 'TARGET' in cols_to_drop:\n",
    "    cols_to_drop.remove('TARGET')\n",
    "\n",
    "print(f\"Dropping {len(cols_to_drop)} columns:\")\n",
    "print(f\"  - {len([c for c in cols_to_drop if c in missing_df[missing_df['Missing_Percentage'] > threshold]['Column'].tolist()])} high missing\")\n",
    "print(f\"  - {len([c for c in cols_to_drop if c in constant_features])} constant\")\n",
    "print(f\"  - {'SK_ID_CURR' if 'SK_ID_CURR' in cols_to_drop else 'None'} (ID column)\")\n",
    "\n",
    "df_cleaned = df.drop(columns=cols_to_drop)\n",
    "print(f\"\\n‚úÖ Shape after cleaning: {df_cleaned.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d440560f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FEATURE ENGINEERING\n",
      "================================================================================\n",
      "‚úÖ Created: income_to_credit_ratio\n",
      "‚úÖ Created: payment_burden\n",
      "‚úÖ Created: dpd_utilization_interaction\n",
      "‚úÖ Created: age_group\n",
      "\n",
      "‚úÖ Final shape: (307511, 63)\n",
      "   Added 4 new features\n"
     ]
    }
   ],
   "source": [
    "# Feature Engineering\n",
    "print(\"=\"*80)\n",
    "print(\"FEATURE ENGINEERING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create ratio features (domain knowledge)\n",
    "if 'raw_income_total' in df_cleaned.columns and 'raw_credit_amt' in df_cleaned.columns:\n",
    "    df_cleaned['income_to_credit_ratio'] = df_cleaned['raw_income_total'] / (df_cleaned['raw_credit_amt'] + 1e-6)\n",
    "    print(\"‚úÖ Created: income_to_credit_ratio\")\n",
    "\n",
    "if 'raw_annuity_amt' in df_cleaned.columns and 'raw_income_total' in df_cleaned.columns:\n",
    "    df_cleaned['payment_burden'] = df_cleaned['raw_annuity_amt'] / (df_cleaned['raw_income_total'] + 1e-6)\n",
    "    print(\"‚úÖ Created: payment_burden\")\n",
    "\n",
    "if 'AMT_CREDIT_SUM_DEBT' in df_cleaned.columns and 'AMT_CREDIT_SUM' in df_cleaned.columns:\n",
    "    df_cleaned['credit_utilization'] = df_cleaned['AMT_CREDIT_SUM_DEBT'] / (df_cleaned['AMT_CREDIT_SUM'] + 1e-6)\n",
    "    print(\"‚úÖ Created: credit_utilization\")\n",
    "\n",
    "# Create aggregation features\n",
    "if 'dpd_mean' in df_cleaned.columns and 'total_utilization' in df_cleaned.columns:\n",
    "    df_cleaned['dpd_utilization_interaction'] = df_cleaned['dpd_mean'] * df_cleaned['total_utilization']\n",
    "    print(\"‚úÖ Created: dpd_utilization_interaction\")\n",
    "\n",
    "# Age groups (if age exists)\n",
    "if 'age_years' in df_cleaned.columns:\n",
    "    df_cleaned['age_group'] = pd.cut(\n",
    "        df_cleaned['age_years'], \n",
    "        bins=[0, 25, 35, 45, 55, 100], \n",
    "        labels=[0, 1, 2, 3, 4]\n",
    "    ).astype(int)\n",
    "    print(\"‚úÖ Created: age_group\")\n",
    "\n",
    "print(f\"\\n‚úÖ Final shape: {df_cleaned.shape}\")\n",
    "print(f\"   Added {df_cleaned.shape[1] - df.drop(columns=cols_to_drop).shape[1]} new features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eaea699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "HANDLING MISSING VALUES\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric features: 62\n",
      "Categorical features: 0\n",
      "\n",
      "Imputation strategy:\n",
      "  Low missing (<20%): 37 columns ‚Üí Median\n",
      "  Medium missing (20-50%): 2 columns ‚Üí KNN\n",
      "\n",
      "üîÑ Applying KNN imputation (this may take a while)...\n"
     ]
    }
   ],
   "source": [
    "# Handle Missing Values Intelligently\n",
    "print(\"=\"*80)\n",
    "print(\"HANDLING MISSING VALUES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Separate features by type\n",
    "X = df_cleaned.drop(columns=['TARGET'])\n",
    "y = df_cleaned['TARGET'].astype(int)\n",
    "\n",
    "# Identify numeric vs categorical (if any)\n",
    "numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_features = X.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "\n",
    "print(f\"Numeric features: {len(numeric_features)}\")\n",
    "print(f\"Categorical features: {len(categorical_features)}\")\n",
    "\n",
    "# Strategy 1: Simple median imputation for features with <20% missing\n",
    "# Strategy 2: KNN imputation for features with 20-50% missing\n",
    "# Strategy 3: Create missing indicator for important features\n",
    "\n",
    "missing_pct = X.isnull().sum() / len(X) * 100\n",
    "\n",
    "low_missing = missing_pct[(missing_pct > 0) & (missing_pct < 20)].index.tolist()\n",
    "medium_missing = missing_pct[(missing_pct >= 20) & (missing_pct <= 50)].index.tolist()\n",
    "\n",
    "print(f\"\\nImputation strategy:\")\n",
    "print(f\"  Low missing (<20%): {len(low_missing)} columns ‚Üí Median\")\n",
    "print(f\"  Medium missing (20-50%): {len(medium_missing)} columns ‚Üí KNN\")\n",
    "\n",
    "# Apply simple imputation first\n",
    "simple_imputer = SimpleImputer(strategy='median')\n",
    "if len(low_missing) > 0:\n",
    "    X[low_missing] = simple_imputer.fit_transform(X[low_missing])\n",
    "\n",
    "# Apply KNN imputation for medium missing (optional - can be slow)\n",
    "if len(medium_missing) > 0 and len(medium_missing) < 50:  # Only if manageable\n",
    "    print(f\"\\nüîÑ Applying KNN imputation (this may take a while)...\")\n",
    "    knn_imputer = KNNImputer(n_neighbors=5, weights='distance')\n",
    "    X[medium_missing] = knn_imputer.fit_transform(X[medium_missing])\n",
    "    print(\"‚úÖ KNN imputation complete\")\n",
    "else:\n",
    "    # Fallback to median for medium missing if too many columns\n",
    "    if len(medium_missing) > 0:\n",
    "        X[medium_missing] = simple_imputer.fit_transform(X[medium_missing])\n",
    "        print(f\"‚ö†Ô∏è Too many columns for KNN, using median imputation instead\")\n",
    "\n",
    "# Final check: fill any remaining NaNs with 0\n",
    "X = X.fillna(0)\n",
    "\n",
    "print(f\"\\n‚úÖ Missing values handled\")\n",
    "print(f\"   Remaining NaNs: {X.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0160788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After SMOTE:\n",
      "TARGET\n",
      "0    0.5\n",
      "1    0.5\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Remove Outliers (Conservative Approach)\n",
    "print(\"=\"*80)\n",
    "print(\"OUTLIER HANDLING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# Identify high-value features that might have outliers\n",
    "outlier_candidates = ['raw_income_total', 'raw_credit_amt', 'raw_annuity_amt']\n",
    "outlier_candidates = [col for col in outlier_candidates if col in X.columns]\n",
    "\n",
    "initial_len = len(X)\n",
    "\n",
    "for col in outlier_candidates:\n",
    "    # Use IQR method (more robust than z-score)\n",
    "    Q1 = X[col].quantile(0.01)  # 1st percentile\n",
    "    Q3 = X[col].quantile(0.99)  # 99th percentile\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    # Only remove extreme outliers\n",
    "    lower_bound = Q1 - 3 * IQR\n",
    "    upper_bound = Q3 + 3 * IQR\n",
    "    \n",
    "    mask = (X[col] >= lower_bound) & (X[col] <= upper_bound)\n",
    "    X = X[mask]\n",
    "    y = y[mask]\n",
    "    \n",
    "    removed = initial_len - len(X)\n",
    "    if removed > 0:\n",
    "        print(f\"  {col}: removed {removed} extreme outliers\")\n",
    "        initial_len = len(X)\n",
    "\n",
    "print(f\"\\n‚úÖ Final data shape: {X.shape}\")\n",
    "print(f\"   Removed {df_cleaned.shape[0] - len(X)} total outliers ({(df_cleaned.shape[0] - len(X))/df_cleaned.shape[0]*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b88672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Feature Scaling (Optional for Tree-based, but helps with stability)\n",
    "print(\"=\"*80)\n",
    "print(\"FEATURE SCALING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Use RobustScaler for high-value features (resistant to outliers)\n",
    "scale_features = ['raw_income_total', 'raw_credit_amt', 'raw_annuity_amt', \n",
    "                  'AMT_CREDIT_SUM', 'AMT_CREDIT_SUM_DEBT']\n",
    "scale_features = [col for col in scale_features if col in X.columns]\n",
    "\n",
    "if len(scale_features) > 0:\n",
    "    scaler = RobustScaler()\n",
    "    X[scale_features] = scaler.fit_transform(X[scale_features])\n",
    "    print(f\"‚úÖ Scaled {len(scale_features)} high-value features\")\n",
    "    \n",
    "    # Save scaler for production\n",
    "    joblib.dump(scaler, r\"C:\\Users\\Asus\\Documents\\GitHub\\Credit-Scoring\\output\\models\\robust_scaler.pkl\")\n",
    "    print(\"‚úÖ Scaler saved for production use\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No features to scale\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a1c722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Test Split (STRATIFIED)\n",
    "print(\"=\"*80)\n",
    "print(\"TRAIN/TEST SPLIT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y  # CRITICAL: Ensures same distribution\n",
    ")\n",
    "\n",
    "print(f\"Train size: {X_train.shape}\")\n",
    "print(f\"Test size: {X_test.shape}\")\n",
    "\n",
    "print(f\"\\nClass distribution (Training):\")\n",
    "print(f\"  Class 0 (No Default): {Counter(y_train)[0]:,} ({Counter(y_train)[0]/len(y_train)*100:.2f}%)\")\n",
    "print(f\"  Class 1 (Default): {Counter(y_train)[1]:,} ({Counter(y_train)[1]/len(y_train)*100:.2f}%)\")\n",
    "print(f\"  Imbalance Ratio: {Counter(y_train)[0]/Counter(y_train)[1]:.2f}:1\")\n",
    "\n",
    "print(f\"\\nClass distribution (Testing):\")\n",
    "print(f\"  Class 0 (No Default): {Counter(y_test)[0]:,} ({Counter(y_test)[0]/len(y_test)*100:.2f}%)\")\n",
    "print(f\"  Class 1 (Default): {Counter(y_test)[1]:,} ({Counter(y_test)[1]/len(y_test)*100:.2f}%)\")\n",
    "print(f\"  Imbalance Ratio: {Counter(y_test)[0]/Counter(y_test)[1]:.2f}:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95855d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Processed Data (WITHOUT SMOTE)\n",
    "print(\"=\"*80)\n",
    "print(\"SAVING PROCESSED DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# IMPORTANT: Save ORIGINAL split, NOT resampled data\n",
    "# SMOTE should be applied during training only!\n",
    "\n",
    "save_path = r\"C:\\Users\\Asus\\Documents\\GitHub\\Credit-Scoring\\output\\models\\processed_data_lgbm_v2.pkl\"\n",
    "\n",
    "joblib.dump(\n",
    "    (X_train, X_test, y_train, y_test),\n",
    "    save_path\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Processed data saved to:\")\n",
    "print(f\"   {save_path}\")\n",
    "\n",
    "# Save metadata\n",
    "metadata = {\n",
    "    'original_shape': df.shape,\n",
    "    'cleaned_shape': df_cleaned.shape,\n",
    "    'final_shape': X.shape,\n",
    "    'features_removed': len(cols_to_drop),\n",
    "    'features_engineered': df_cleaned.shape[1] - df.drop(columns=cols_to_drop).shape[1],\n",
    "    'outliers_removed': df_cleaned.shape[0] - len(X),\n",
    "    'train_shape': X_train.shape,\n",
    "    'test_shape': X_test.shape,\n",
    "    'class_imbalance_train': Counter(y_train)[0] / Counter(y_train)[1],\n",
    "    'class_imbalance_test': Counter(y_test)[0] / Counter(y_test)[1],\n",
    "    'feature_names': X.columns.tolist(),\n",
    "    'scaled_features': scale_features if len(scale_features) > 0 else []\n",
    "}\n",
    "\n",
    "joblib.dump(metadata, r\"C:\\Users\\Asus\\Documents\\GitHub\\Credit-Scoring\\output\\models\\data_metadata_v2.pkl\")\n",
    "\n",
    "print(f\"\\n‚úÖ Metadata saved\")\n",
    "print(f\"\\nüìä Data Processing Summary:\")\n",
    "print(f\"   Original features: {df.shape[1]}\")\n",
    "print(f\"   Features removed: {len(cols_to_drop)}\")\n",
    "print(f\"   Features engineered: {metadata['features_engineered']}\")\n",
    "print(f\"   Final features: {X.shape[1]}\")\n",
    "print(f\"   Samples removed (outliers): {metadata['outliers_removed']}\")\n",
    "print(f\"   Train samples: {X_train.shape[0]:,}\")\n",
    "print(f\"   Test samples: {X_test.shape[0]:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74325784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation - Check Data Quality\n",
    "print(\"=\"*80)\n",
    "print(\"DATA QUALITY VALIDATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Check for any remaining issues\n",
    "print(\"‚úì Checks:\")\n",
    "print(f\"  Missing values in train: {X_train.isnull().sum().sum()}\")\n",
    "print(f\"  Missing values in test: {X_test.isnull().sum().sum()}\")\n",
    "print(f\"  Infinite values in train: {np.isinf(X_train).sum().sum()}\")\n",
    "print(f\"  Infinite values in test: {np.isinf(X_test).sum().sum()}\")\n",
    "print(f\"  Duplicate rows in train: {X_train.duplicated().sum()}\")\n",
    "print(f\"  Duplicate rows in test: {X_test.duplicated().sum()}\")\n",
    "\n",
    "# Check feature variance\n",
    "low_variance_features = X_train.columns[X_train.var() < 0.01].tolist()\n",
    "if len(low_variance_features) > 0:\n",
    "    print(f\"\\n‚ö†Ô∏è Warning: {len(low_variance_features)} features have very low variance\")\n",
    "    print(f\"   Consider removing: {low_variance_features[:5]}\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ All features have sufficient variance\")\n",
    "\n",
    "print(f\"\\n‚úÖ Data quality validation complete!\")\n",
    "print(f\"   Ready for model training\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
